<!--- SPDX-License-Identifier: Apache-2.0 -->

# Wed Aug 11, 2021 at 3:00pm UTC

## Agenda
* Introduction
  * Member introductions
  * Meeting date/time and frequency
  * Preferred method of communication
* Define problem statement
  * Motivation
  * Gathering suggestions
  * Arrive at a consensus
* Define action items for the next meeting

## Attendees
* Joaquin Anton (NVIDIA)
* Janusz Lisiecki (NVIDIA)
* Michal Szolucha (NVIDIA)
* Mayank Kaushik (NVIDIA)
* Ashwini Khade (Microsoft)
* Rodolfo G Esteves (Intel)
* Ganesan Ramalingan (Microsoft)
* Jacky Chen (Microsoft)
* Matteo Interlandi (Microsoft)
* Prasanth Pulavarthi (Microsoft)
* Ohara Moriyoshi (IBM)

## Recording & Slides

[Meeting Recording](https://lists.lfaidata.foundation/g/onnx-wg-preprocessing/files/ONNX%20pre_post%20processing%20and%20featurization%20WG%20kick-off%20meting-20210811_180626-Meeting%20Recording.mp4)
[Slides](slides/20210811_slides.pdf)

## Meeting Notes

* We agreed to meet monthly, Wednesday (or Tuesday-Thursday) at 5 PM CEST, to accommodate different time-zones, as we have people joining from America, Europe, and Asia.
* We agreed to use Slack as a main communication channel.
* There is a lack of standardization in data preprocessing. Different libraries are used, leading to portability issues.
* The initial goal of the group is to make data preprocessing part of ONNX and come up with a standardized set of operators that will support training and inference for a set of selected networks. The plan is to first standardize preprocessing, then incorporate it into the model.
* We see that there are existing ONNX operators that could be used to define a data preprocessing pipeline. The challenge appears with the different nature of the preprocessing pipeline. A preprocessing pipeline is typically defined on a per-sample basis, and it’s part of the data loader to assemble the preprocessed samples to a uniform batch that is then fed to the model. We don’t yet have a good idea on how the data preprocessing part can be integrated with the ONNX graph, due to its different nature.
* ONNX has no notion of batch in tensors, but the batch can be one of the dimensions. ONNX has a sequence of tensors that could be used to represent a non-uniform batch in the preprocessing pipeline.
* Data preprocessing differs from training to inference. How to incorporate that into the graph?
* Exporting to ONNX graph: Should we only support exporting TF/PyTorch/etc data pre-preprocessing to ONNX or should we also cover usage of other FWs/libraries (e.g. Pillow)?
* How will the runtime implement preprocessing? Implementations are meant to work on big tensors. We might need to define data preprocessing as functions that the runtime could optimize.

## Action items:
* **Everybody** A member of each organization to come up with a sorted list of most important networks that we would like to support initially
* **Everybody** Think about how the data processing pipeline could be represented and connected to the model
