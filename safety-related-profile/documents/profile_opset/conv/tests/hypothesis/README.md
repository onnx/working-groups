# How to run
There are two main scripts, test_conv.py and check_edges.py

Before run any of those scripts you need to have the requirements that are presented at requirements.txt

To run the test_conv.py you need to do
```bash
pytest test_conv.py
```
Or if you want to see more information about the tests
```bash
pytest -s test_conv.py
```
This script will generate a json file with all the tests that were generated.

To run check_edges.py you need to do
```bash
python check_edges.py
```
Make sure that you have the json file generated by test_conv.py

# Hypothesis Tests Doubts for Conv operator

## Conv

### Inputs/attributes generation
What would be the best approach for generating inputs that satisfy the **X[C3]** constraint?

We know that ideally Hypothesis should generate input values that already satisfied this equation. We have been searching for ways to do that and we found no viable way to do so. 

> (eric) I have not checked it very carefully, but Hypothesis supports several "backends", including one ("crosshair") that uses a SMT Solver.
> (eric later...) I have tried to use the backend (in a Google collab notebook), with no success. I have had no time to test it directly. Note also that [crosshair] is a standalone tool. But multiplying the tools is not a good idea in any case.
> So we have to stick to a handcrafted test generation strategy with filtering. That is what you've done and that seems fine to me!

Therefore, our approach was to generate two unconstrained variables, and then derive the remaining two based on those, ensuring the final inputs satisfy the required condition.

> (eric) Yes. This makes sense. The main difference with a strategy where the constraints would be solved resides in the fact that we have to process variables in a "smart" way in order to avoid generating too many invalid test cases. For instance if the constraint is $a+40xb<100$ with $a$ and $b$ in $[0,100]$, it is wise to draw $a$ first and solve for $b$, otherwise, you may generate a lot of invalid cases... And, in the worst case, you may well never exercise some values of $a$. Well, there is probably some strategy to be defined.
> (eric) To be honest, I  am not completely convinced by this random testing approach if it does not really simply our work (i.e., find tricky test cases automatically). Our operators are simple and it does not seem very smart to generate 100 different tests for (e.g.,) $a+b$ with $a$ and and $b$ in $[0,1000] while we know that once you have tested it with all edge and corner cases plus a another non edge case, and all test pass, it is very likely that any test will pass too (hypothesis of equivalence classes). So, basically, the strategy should consist in (i) defining the equivalence classes, (ii) writing one test case per equivalence class, leaving random testing for problems where such equivalence class cannot not be defined easily (and we hope that random sampling will pick one element in each (undefined) class...)
> (eric) => We should probably have a plenary discussion about our test strategy... 

> (eric) btw, there is a custom-defined strategy called [hypothesis-torch](https://pypi.org/project/hypothesis-torch/). Might be useful (?). 

 Another possibility was to generate only unconstrainted values and then filter by those who satisfy the equation (this would be computationally expensive and would probably defeat the purpose of using Hypothesis â€” many edge cases would be lost).

> I agree. Filtering does not seem to be a good idea... 


### Auto_Pad = "Valid"
There is no documentation stating how this type of configuration works.

> (eric)  In the doc, I read "VALID mean no padding". So I guess that this means the the size of the input tensor and the size of the kernel must be such that the kernel fits within the tensor. 

Therefore we are unable to calculate dY dimensions (useful for the generation phase) and we cannot reason about the results it produces.
How should we proceed?
Should we ignore this configuration for Auto_Pad or is there any documentation that we haven't looked yet.

> (eric) No, you should consider it, but it simply corresponds to padding equal to zero at top, bottom, left, right... Notet that we may also reject this configuration considering that it is basically a non explicit padding. But let's consider it (since you have already basically treated this case).

# Explanations of Conv corner and edge cases generation

## Assumptions
First of all let us explain what we mean by corner and edge cases.
- **Corner cases** are the extreme values of a given parameter, such as the minimum and maximum values that the parameter can take.

- **Edge cases** are values that are just above or below the minimum and maximum values, respectively.

> (eric) In my mind, a "edge" case correspond to the extreme limit of an argument  (its min and max). A "corner" case is the combination of edge cases for multiple parameters (hence the name "corner").  

## Conv tests
For concat operator we checked the **corner and edge cases** for: **dx0, dx1, dx2, dx3, dw0, strides, auto_pad and pads**.
We think its **irrelevant** to check the corner and edge cases for **db0** because of **B [C1]** and **kernel shape** because of **W [C3]**.
We didn't check the corner and edges cases for **dw2, dw3 and dilations** because the way we generate them is not independent. 

> (eric) I agree, some combination of edge cases are not in the domain. For instance, if my constraint is x+y < 10 with x in [0,10] and y in [5,10], some corner cases are invalid.  

We consider that **dx0, dx1, and dw0** as lines since they are independent and we check the corner and edge cases for them.

For **dx2** and **dx3** we consider them as a **rectangle**. We check each **corner** of the rectangle, a **middle point of each rectangle edge** and a **middle point of the rectangle**.

> (eric) We can consider that those cases have already been "captured" by  standard (non edge/corner) tests.

For **pads** (when **auto_pad == NOTSET**) we consider a **4D cube** and we check each corner of the "4D cube". All **16 corners**, example, (min,min,min,min), (min,min,min,max), ... (min,max,max,max), (max,max,max,max) where min is the minimum value of pads and max is the maximum value of pads.
Also we check a middle point of each edge of the "4D cube" and a middle point of the "4D cube".

For **auto_pad** we check if all the possible values are being generated (Except **VALID**)

> (eric) Yes, but as written before, you may still consider "VALID". 

For **strides** we consider them as a **rectangle***. We check each **corner** of the rectangle, a **middle point of each rectangle edge** and a **middle point of the rectangle**.

> (eric) Sounds good.
> Writing a few line on the overall testing approach would probably be useful. Another possibility would be to write a siple example (with the two python scripts, one for the standard tests and the other for the corner cases) that would serve both as a template and a documentation.
> BTW, I would have expected Hypothesis to capture some of the edge cases...