{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e420862",
   "metadata": {},
   "source": [
    "This IS a Test of the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db948602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 18:16:45.992852: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-16 18:16:46.031872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-16 18:16:46.875208: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12\n",
      "ONNX Version: 1.17.0\n",
      "\n",
      "--- PyTorch ---\n",
      "Version: 2.9.1+cu128\n",
      "CUDA disponible: True\n",
      "GPU détecté: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Test multiplication matrice PyTorch OK, somme: -201616.28125\n",
      "\n",
      "--- TensorFlow ---\n",
      "Version: 2.20.0\n",
      "GPU TensorFlow détecté: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Test multiplication matrice TF OK, somme: 115546.414\n",
      "\n",
      "--- ONNX Runtime ---\n",
      "Version: 1.23.2\n",
      "Providers disponibles: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "--- Test ONNX 'Where' operator ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771262207.284949   22516 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13888 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider TensorrtExecutionProvider: ✅ Test Where OK, sortie: [ 1. 20.  3.]\n",
      "Provider CUDAExecutionProvider: ✅ Test Where OK, sortie: [ 1. 20.  3.]\n",
      "Provider CPUExecutionProvider: ✅ Test Where OK, sortie: [ 1. 20.  3.]\n",
      "\n",
      "--- Comparaison NumPy ---\n",
      "Résultat NumPy: [ 1. 20.  3.]\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, tensorflow as tf, onnxruntime as ort, numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "\n",
    "# --- Versions Python & ONNX ---\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"ONNX Version:\", onnx.__version__)\n",
    "\n",
    "# --- PyTorch ---\n",
    "print(\"\\n--- PyTorch ---\")\n",
    "print(\"Version:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU détecté:\", torch.cuda.get_device_name(0))\n",
    "    x = torch.randn(4096, 4096, device=\"cuda\")\n",
    "    y = x @ x\n",
    "    print(\"Test multiplication matrice PyTorch OK, somme:\", y.sum().item())\n",
    "\n",
    "# --- TensorFlow ---\n",
    "print(\"\\n--- TensorFlow ---\")\n",
    "print(\"Version:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU TensorFlow détecté:\", gpus)\n",
    "if gpus:\n",
    "    a = tf.random.normal([4096,4096])\n",
    "    b = tf.random.normal([4096,4096])\n",
    "    c = tf.matmul(a,b)\n",
    "    print(\"Test multiplication matrice TF OK, somme:\", tf.reduce_sum(c).numpy())\n",
    "\n",
    "# --- ONNX Runtime ---\n",
    "print(\"\\n--- ONNX Runtime ---\")\n",
    "print(\"Version:\", ort.__version__)\n",
    "print(\"Providers disponibles:\", ort.get_available_providers())\n",
    "\n",
    "# --- Test ONNX 'Where' operator ---\n",
    "print(\"\\n--- Test ONNX 'Where' operator ---\")\n",
    "C = helper.make_tensor_value_info(\"C\", TensorProto.BOOL, [3])\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [3])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [3])\n",
    "Z = helper.make_tensor_value_info(\"Z\", TensorProto.FLOAT, [3])\n",
    "node = helper.make_node(\"Where\", [\"C\",\"X\",\"Y\"], [\"Z\"])\n",
    "graph = helper.make_graph([node], \"where_graph\", [C,X,Y], [Z])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 23)])\n",
    "save(model, \"where_test.onnx\")\n",
    "\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"where_test.onnx\", providers=[provider])\n",
    "        cond = np.array([True, False, True], dtype=bool)\n",
    "        x = np.array([1.0,2.0,3.0], dtype=np.float32)\n",
    "        y = np.array([10.0,20.0,30.0], dtype=np.float32)\n",
    "        z = sess.run(None, {\"C\":cond, \"X\":x, \"Y\":y})[0]\n",
    "        print(f\"Provider {provider}: ✅ Test Where OK, sortie:\", z)\n",
    "    except Exception as e:\n",
    "        print(f\"Provider {provider}: ❌ Erreur:\", e)\n",
    "\n",
    "# --- Comparaison NumPy ---\n",
    "print(\"\\n--- Comparaison NumPy ---\")\n",
    "cond = np.array([True, False, True])\n",
    "x = np.array([1.0,2.0,3.0])\n",
    "y = np.array([10.0,20.0,30.0])\n",
    "z_np = np.where(cond,x,y)\n",
    "print(\"Résultat NumPy:\", z_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0fb84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test ONNX 'MaxPool' operator Nominal ---\n",
      "\n",
      "--- Rien de special a voir tout est OK, avec CPU/CUDA meme resultat ---\n",
      "\n",
      "Provider TensorrtExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0 6]\n",
      "   [8 8]]]]\n",
      "Indices =\n",
      " [[[[-4  5]\n",
      "   [ 7  7]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0 6]\n",
      "   [8 8]]]]\n",
      "Indices =\n",
      " [[[[-4  5]\n",
      "   [ 7  7]]]]\n",
      "\n",
      "Provider CPUExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0 6]\n",
      "   [8 8]]]]\n",
      "Indices =\n",
      " [[[[-4  5]\n",
      "   [ 7  7]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2026-02-16 18:16:48.009336902 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:135: While parsing node number 0 [MaxPool -> \"Y\"]:\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.013163081 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "output: \"Indices\"\n",
      "name: \"\"\n",
      "op_type: \"MaxPool\"\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 2\n",
      "  ints: 2\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"ceil_mode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"storage_order\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  s: \"NOTSET\"\n",
      "  type: STRING\n",
      "}\n",
      "\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.013171022 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:139: --- End node ---\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.013175856 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
      "[8] This version of TensorRT doesn't support mode than 1 outputs for MaxPool nodes!\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.013226412 [W:onnxruntime:Default, tensorrt_execution_provider.cc:2833 GetCapability] [TensorRT EP] No graph will run on TensorRT execution provider\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "\n",
    "# --- Test ONNX 'MaxPool' operator Nominal ---\n",
    "print(\"\\n--- Test ONNX 'MaxPool' operator Nominal ---\")\n",
    "print(\"\\n--- Rien de special a voir tout est OK, avec CPU/CUDA meme resultat ---\")\n",
    "\n",
    "onnx_type = TensorProto.UINT8\n",
    "\n",
    "# Input / Output shapes\n",
    "X = helper.make_tensor_value_info(\"X\", onnx_type, [1, 1, 3, 3])\n",
    "Y = helper.make_tensor_value_info(\"Y\", onnx_type, [1, 1, 2, 2])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 2, 2])\n",
    "\n",
    "# MaxPool node (avec 2 sorties)\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[0, 0, 0, 0],\n",
    "    strides=[1, 1],\n",
    "    dilations=[1, 1],\n",
    "    ceil_mode=0\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_graph_3x3\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 22)])\n",
    "save(model, \"maxpool_test.onnx\")\n",
    "\n",
    "# Données demandées\n",
    "data = [[[[ 0, 0, 5],\n",
    "          [ 0, 0, 6],\n",
    "          [ 7, 8, 0]]]]\n",
    "\n",
    "x = np.array(data, dtype=np.uint8)\n",
    "\n",
    "# Test pour tous les providers\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"maxpool_test.onnx\", providers=[provider])\n",
    "\n",
    "        outputs = sess.run(None, {\"X\": x})\n",
    "        y = outputs[0]\n",
    "        indices = outputs[1]\n",
    "\n",
    "        print(f\"\\nProvider {provider}: ✅ Test MaxPool OK\")\n",
    "        print(\"Sortie Y =\\n\", y)\n",
    "        print(\"Indices =\\n\", indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProvider {provider}: ❌ Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907bbaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test ONNX 'MaxPool' operator (DOUBLE + padding + indices) ---\n",
      "\n",
      "--- voir le -4 (quand padding), avec CPU/CUDA meme resultat ---\n",
      "\n",
      "Provider TensorrtExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[ 0.00000000e+000  9.57875561e+000  9.57875561e+000  4.56432533e+000]\n",
      "   [ 2.72844928e+000  9.57875561e+000  9.57875561e+000  4.56432533e+000]\n",
      "   [ 2.83691720e+000  3.54234851e+000  3.54234851e+000  2.55354471e+000]\n",
      "   [ 2.83691720e+000  3.46789489e+000  3.46789489e+000 -1.79769313e+308]]]]\n",
      "Indices =\n",
      " [[[[ 0  1  1  2]\n",
      "   [ 3  1  1  2]\n",
      "   [ 6  4  4  5]\n",
      "   [ 6  7  7 -4]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[ 0.00000000e+000  9.57875561e+000  9.57875561e+000  4.56432533e+000]\n",
      "   [ 2.72844928e+000  9.57875561e+000  9.57875561e+000  4.56432533e+000]\n",
      "   [ 2.83691720e+000  3.54234851e+000  3.54234851e+000  2.55354471e+000]\n",
      "   [ 2.83691720e+000  3.46789489e+000  3.46789489e+000 -1.79769313e+308]]]]\n",
      "Indices =\n",
      " [[[[ 0  1  1  2]\n",
      "   [ 3  1  1  2]\n",
      "   [ 6  4  4  5]\n",
      "   [ 6  7  7 -4]]]]\n",
      "\n",
      "Provider CPUExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[ 0.00000000e+000  9.57875561e+000  9.57875561e+000  4.56432533e+000]\n",
      "   [ 2.72844928e+000  9.57875561e+000  9.57875561e+000  4.56432533e+000]\n",
      "   [ 2.83691720e+000  3.54234851e+000  3.54234851e+000  2.55354471e+000]\n",
      "   [ 2.83691720e+000  3.46789489e+000  3.46789489e+000 -1.79769313e+308]]]]\n",
      "Indices =\n",
      " [[[[ 0  1  1  2]\n",
      "   [ 3  1  1  2]\n",
      "   [ 6  4  4  5]\n",
      "   [ 6  7  7 -4]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2026-02-16 18:16:48.038449443 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:135: While parsing node number 0 [MaxPool -> \"Y\"]:\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.038494919 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "output: \"Indices\"\n",
      "name: \"\"\n",
      "op_type: \"MaxPool\"\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 2\n",
      "  ints: 2\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"ceil_mode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"storage_order\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  s: \"NOTSET\"\n",
      "  type: STRING\n",
      "}\n",
      "\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.038499565 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:139: --- End node ---\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.038503576 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
      "[8] This version of TensorRT doesn't support mode than 1 outputs for MaxPool nodes!\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.038534669 [W:onnxruntime:Default, tensorrt_execution_provider.cc:2833 GetCapability] [TensorRT EP] No graph will run on TensorRT execution provider\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "\n",
    "# --- Test ONNX 'MaxPool' operator (DOUBLE + padding + indices) ---\n",
    "print(\"\\n--- Test ONNX 'MaxPool' operator (DOUBLE + padding + indices) ---\")\n",
    "print(\"\\n--- voir le -4 (quand padding), avec CPU/CUDA meme resultat ---\")\n",
    "\n",
    "onnx_type = TensorProto.DOUBLE\n",
    "\n",
    "# Input / Output shapes\n",
    "X = helper.make_tensor_value_info(\"X\", onnx_type, [1, 1, 3, 3])\n",
    "Y = helper.make_tensor_value_info(\"Y\", onnx_type, [1, 1, 4, 4])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 4, 4])\n",
    "\n",
    "# MaxPool node\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1],\n",
    "    dilations=[1, 1],\n",
    "    ceil_mode=0\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_graph_double_pad\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 22)])\n",
    "save(model, \"maxpool_test.onnx\")\n",
    "\n",
    "# Données demandées\n",
    "data = [[[[0.0, 9.57875561, 4.56432533],\n",
    "          [2.72844928, 3.54234851, 2.55354471],\n",
    "          [2.83691720, 3.46789489, -np.inf]]]]\n",
    "\n",
    "x = np.array(data, dtype=np.float64)\n",
    "\n",
    "# Test pour tous les providers\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"maxpool_test.onnx\", providers=[provider])\n",
    "\n",
    "        outputs = sess.run(None, {\"X\": x})\n",
    "        y = outputs[0]\n",
    "        indices = outputs[1]\n",
    "\n",
    "        print(f\"\\nProvider {provider}: ✅ Test MaxPool OK\")\n",
    "        print(\"Sortie Y =\\n\", y)\n",
    "        print(\"Indices =\\n\", indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProvider {provider}: ❌ Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a965461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test ONNX 'MaxPool' operator pour obtenir indice -3 ---\n",
      "\n",
      "--- voir le -4 (quand padding), avec CPU/CUDA mais divergence cette fois ---\n",
      "\n",
      "Provider TensorrtExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[ 0.0000000e+00  0.0000000e+00 -3.4028235e+38]\n",
      "   [ 1.0000000e+00  1.1000000e+00  1.1000000e+00]\n",
      "   [ 1.0000000e+00  1.1000000e+00  1.1000000e+00]]]]\n",
      "Indices =\n",
      " [[[[ 0  0 -3]\n",
      "   [ 2  3  3]\n",
      "   [ 2  3  3]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0.  0.  0. ]\n",
      "   [1.  1.1 1.1]\n",
      "   [1.  1.1 1.1]]]]\n",
      "Indices =\n",
      " [[[[ 0  0 -4]\n",
      "   [ 2  3  3]\n",
      "   [ 2  3  3]]]]\n",
      "\n",
      "Provider CPUExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[ 0.0000000e+00  0.0000000e+00 -3.4028235e+38]\n",
      "   [ 1.0000000e+00  1.1000000e+00  1.1000000e+00]\n",
      "   [ 1.0000000e+00  1.1000000e+00  1.1000000e+00]]]]\n",
      "Indices =\n",
      " [[[[ 0  0 -3]\n",
      "   [ 2  3  3]\n",
      "   [ 2  3  3]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2026-02-16 18:16:48.064136547 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:135: While parsing node number 0 [MaxPool -> \"Y\"]:\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.064180677 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "output: \"Indices\"\n",
      "name: \"\"\n",
      "op_type: \"MaxPool\"\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 2\n",
      "  ints: 2\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"ceil_mode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"storage_order\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  s: \"NOTSET\"\n",
      "  type: STRING\n",
      "}\n",
      "\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.064186077 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:139: --- End node ---\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.064190755 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
      "[8] This version of TensorRT doesn't support mode than 1 outputs for MaxPool nodes!\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.064227662 [W:onnxruntime:Default, tensorrt_execution_provider.cc:2833 GetCapability] [TensorRT EP] No graph will run on TensorRT execution provider\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"\\n--- Test ONNX 'MaxPool' operator pour obtenir indice -3 ---\")\n",
    "print(\"\\n--- voir le -4 (quand padding), avec CPU/CUDA mais divergence cette fois ---\")\n",
    "\n",
    "onnx_type = TensorProto.FLOAT\n",
    "\n",
    "# Input / Output shapes\n",
    "X = helper.make_tensor_value_info(\"X\", onnx_type, [1, 1, 2, 2])\n",
    "Y = helper.make_tensor_value_info(\"Y\", onnx_type, [1, 1, 3, 3])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 3, 3])\n",
    "\n",
    "# MaxPool node (2x2 kernel, stride 1, padding 1)\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1],\n",
    "    dilations=[1, 1],\n",
    "    ceil_mode=0\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_graph_neg3\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_neg3.onnx\")\n",
    "\n",
    "# Données d'entrée (2x2)\n",
    "data = [[[[0.0, -np.inf],  # placer -inf dans un coin pour forcer un indice négatif\n",
    "          [1.0, 1.1]]]]\n",
    "\n",
    "x = np.array(data, dtype=np.float32)\n",
    "\n",
    "# Test pour tous les providers disponibles\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"maxpool_neg3.onnx\", providers=[provider])\n",
    "\n",
    "        outputs = sess.run(None, {\"X\": x})\n",
    "        y = outputs[0]\n",
    "        indices = outputs[1]\n",
    "\n",
    "        print(f\"\\nProvider {provider}: ✅ Test MaxPool OK\")\n",
    "        print(\"Sortie Y =\\n\", y)\n",
    "        print(\"Indices =\\n\", indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProvider {provider}: ❌ Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7202e2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test ONNX 'MaxPool' operator ---\n",
      "\n",
      "--- voir le -5 (quand padding), avec CPU/CUDA mais divergence cette fois ---\n",
      "\n",
      "Provider TensorrtExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[-3.4028235e+38  1.0000000e-01  2.0000000e-01  3.0000001e-01\n",
      "     3.0000001e-01]\n",
      "   [ 1.0000000e+00  1.1000000e+00  1.2000000e+00  1.3000000e+00\n",
      "     1.3000000e+00]\n",
      "   [ 2.0000000e+00  2.0999999e+00  2.2000000e+00  2.3000000e+00\n",
      "     2.3000000e+00]\n",
      "   [ 2.0000000e+00  2.0999999e+00  2.2000000e+00  2.3000000e+00\n",
      "     2.3000000e+00]]]]\n",
      "Indices =\n",
      " [[[[-5  1  2  3  3]\n",
      "   [ 4  5  6  7  7]\n",
      "   [ 8  9 10 11 11]\n",
      "   [ 8  9 10 11 11]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0.  0.1 0.2 0.3 0.3]\n",
      "   [1.  1.1 1.2 1.3 1.3]\n",
      "   [2.  2.1 2.2 2.3 2.3]\n",
      "   [2.  2.1 2.2 2.3 2.3]]]]\n",
      "Indices =\n",
      " [[[[-6  1  2  3  3]\n",
      "   [ 4  5  6  7  7]\n",
      "   [ 8  9 10 11 11]\n",
      "   [ 8  9 10 11 11]]]]\n",
      "\n",
      "Provider CPUExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[-3.4028235e+38  1.0000000e-01  2.0000000e-01  3.0000001e-01\n",
      "     3.0000001e-01]\n",
      "   [ 1.0000000e+00  1.1000000e+00  1.2000000e+00  1.3000000e+00\n",
      "     1.3000000e+00]\n",
      "   [ 2.0000000e+00  2.0999999e+00  2.2000000e+00  2.3000000e+00\n",
      "     2.3000000e+00]\n",
      "   [ 2.0000000e+00  2.0999999e+00  2.2000000e+00  2.3000000e+00\n",
      "     2.3000000e+00]]]]\n",
      "Indices =\n",
      " [[[[-5  1  2  3  3]\n",
      "   [ 4  5  6  7  7]\n",
      "   [ 8  9 10 11 11]\n",
      "   [ 8  9 10 11 11]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-02-16 18:16:48.091550721 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Y' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.091570620 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Indices' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.093533211 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:135: While parsing node number 0 [MaxPool -> \"Y\"]:\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.093567572 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "output: \"Indices\"\n",
      "name: \"\"\n",
      "op_type: \"MaxPool\"\n",
      "attribute {\n",
      "  name: \"storage_order\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 2\n",
      "  ints: 2\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"ceil_mode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  s: \"NOTSET\"\n",
      "  type: STRING\n",
      "}\n",
      "\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.093572157 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:139: --- End node ---\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.093576515 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
      "[8] This version of TensorRT doesn't support mode than 1 outputs for MaxPool nodes!\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.093614710 [W:onnxruntime:Default, tensorrt_execution_provider.cc:2833 GetCapability] [TensorRT EP] No graph will run on TensorRT execution provider\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.095508613 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Y' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.095522494 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Indices' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.105830092 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Y' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.105844133 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Indices' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"\\n--- Test ONNX 'MaxPool' operator ---\")\n",
    "print(\"\\n--- voir le -5 (quand padding), avec CPU/CUDA mais divergence cette fois ---\")\n",
    "\n",
    "onnx_type = TensorProto.FLOAT\n",
    "\n",
    "# Input / Output shapes\n",
    "# On met une dimension H=3, W=4 pour matcher les données\n",
    "X = helper.make_tensor_value_info(\"X\", onnx_type, [1, 1, 3, 4])\n",
    "# On met des dimensions dynamiques pour laisser ONNX calculer la sortie\n",
    "Y = helper.make_tensor_value_info(\"Y\", onnx_type, [1, 1, 3, 4])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 3, 4])\n",
    "\n",
    "# MaxPool node (2x2 kernel, stride 1, padding 1)\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1],\n",
    "    ceil_mode=0\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_graph_corrige\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_corrige.onnx\")\n",
    "\n",
    "# Données d'entrée (1,1,3,4)\n",
    "data = [[[[ -np.inf, 0.1, 0.2, 0.3],\n",
    "           [ 1.0, 1.1, 1.2, 1.3],\n",
    "           [ 2.0, 2.1, 2.2, 2.3]]]]\n",
    "\n",
    "x = np.array(data, dtype=np.float32)\n",
    "\n",
    "# Test pour tous les providers disponibles\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"maxpool_corrige.onnx\", providers=[provider])\n",
    "\n",
    "        outputs = sess.run(None, {\"X\": x})\n",
    "        y = outputs[0]\n",
    "        indices = outputs[1]\n",
    "\n",
    "        print(f\"\\nProvider {provider}: ✅ Test MaxPool OK\")\n",
    "        print(\"Sortie Y =\\n\", y)\n",
    "        print(\"Indices =\\n\", indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProvider {provider}: ❌ Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0529907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test ONNX 'MaxPool' triple divergence indice et valeur ---\n",
      "\n",
      "--- voir le -3/-4 et le 3/-4 (quand padding), avec CPU/CUDA double divergence cette fois ---\n",
      "\n",
      "--- voir valeur de Y qui diverge aussi inf vs 0 ---\n",
      "\n",
      "Provider TensorrtExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]\n",
      "   [-3.4028235e+38  1.1000000e+00  1.1000000e+00]\n",
      "   [-3.4028235e+38  1.1000000e+00  1.1000000e+00]]]]\n",
      "Indices =\n",
      " [[[[-3 -3 -3]\n",
      "   [-3  3  3]\n",
      "   [-3  3  3]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0.  0.  0. ]\n",
      "   [0.  0.  0. ]\n",
      "   [0.  0.  1.1]]]]\n",
      "Indices =\n",
      " [[[[-4 -4 -4]\n",
      "   [-4 -4 -4]\n",
      "   [-4 -4  3]]]]\n",
      "\n",
      "Provider CPUExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]\n",
      "   [-3.4028235e+38  1.1000000e+00  1.1000000e+00]\n",
      "   [-3.4028235e+38  1.1000000e+00  1.1000000e+00]]]]\n",
      "Indices =\n",
      " [[[[-3 -3 -3]\n",
      "   [-3  3  3]\n",
      "   [-3  3  3]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2026-02-16 18:16:48.122995266 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:135: While parsing node number 0 [MaxPool -> \"Y\"]:\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.123044734 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "output: \"Indices\"\n",
      "name: \"\"\n",
      "op_type: \"MaxPool\"\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 2\n",
      "  ints: 2\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"ceil_mode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"storage_order\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  s: \"NOTSET\"\n",
      "  type: STRING\n",
      "}\n",
      "\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.123049885 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:139: --- End node ---\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.123054832 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
      "[8] This version of TensorRT doesn't support mode than 1 outputs for MaxPool nodes!\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.123090908 [W:onnxruntime:Default, tensorrt_execution_provider.cc:2833 GetCapability] [TensorRT EP] No graph will run on TensorRT execution provider\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"\\n--- Test ONNX 'MaxPool' triple divergence indice et valeur ---\")\n",
    "print(\"\\n--- voir le -3/-4 et le 3/-4 (quand padding), avec CPU/CUDA double divergence cette fois ---\")\n",
    "print(\"\\n--- voir valeur de Y qui diverge aussi inf vs 0 ---\")\n",
    "\n",
    "onnx_type = TensorProto.FLOAT\n",
    "\n",
    "# Input / Output shapes\n",
    "X = helper.make_tensor_value_info(\"X\", onnx_type, [1, 1, 2, 2])\n",
    "Y = helper.make_tensor_value_info(\"Y\", onnx_type, [1, 1, 3, 3])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 3, 3])\n",
    "\n",
    "# MaxPool node (2x2 kernel, stride 1, padding 1)\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1],\n",
    "    dilations=[1, 1],\n",
    "    ceil_mode=0\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_graph_neg3\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_neg3.onnx\")\n",
    "\n",
    "# Données d'entrée (2x2)\n",
    "data = [[[[np.nan, np.nan],  # placer -inf dans un coin pour forcer un indice négatif\n",
    "          [np.nan, 1.1]]]]\n",
    "\n",
    "x = np.array(data, dtype=np.float32)\n",
    "\n",
    "# Test pour tous les providers disponibles\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"maxpool_neg3.onnx\", providers=[provider])\n",
    "\n",
    "        outputs = sess.run(None, {\"X\": x})\n",
    "        y = outputs[0]\n",
    "        indices = outputs[1]\n",
    "\n",
    "        print(f\"\\nProvider {provider}: ✅ Test MaxPool OK\")\n",
    "        print(\"Sortie Y =\\n\", y)\n",
    "        print(\"Indices =\\n\", indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProvider {provider}: ❌ Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f048be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test ONNX 'MaxPool' operator  ---\n",
      "\n",
      "--- voir le valeur et indice avec CPU/CUDA double divergence autre exemple ---\n",
      "\n",
      "Provider TensorrtExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      "    -3.4028235e+38]\n",
      "   [-3.4028235e+38  0.0000000e+00  0.0000000e+00 -3.4028235e+38\n",
      "    -3.4028235e+38]\n",
      "   [-3.4028235e+38  0.0000000e+00  0.0000000e+00 -3.4028235e+38\n",
      "    -3.4028235e+38]\n",
      "   [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      "    -3.4028235e+38]]]]\n",
      "Indices =\n",
      " [[[[-5 -5 -5 -5 -5]\n",
      "   [-5  5  5 -5 -5]\n",
      "   [-5  5  5 -5 -5]\n",
      "   [-5 -5 -5 -5 -5]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n",
      "Indices =\n",
      " [[[[-6 -6 -6 -6 -6]\n",
      "   [-6 -6 -6 -6 -6]\n",
      "   [-6 -6  5 -6 -6]\n",
      "   [-6 -6 -6 -6 -6]]]]\n",
      "\n",
      "Provider CPUExecutionProvider: ✅ Test MaxPool OK\n",
      "Sortie Y =\n",
      " [[[[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      "    -3.4028235e+38]\n",
      "   [-3.4028235e+38  0.0000000e+00  0.0000000e+00 -3.4028235e+38\n",
      "    -3.4028235e+38]\n",
      "   [-3.4028235e+38  0.0000000e+00  0.0000000e+00 -3.4028235e+38\n",
      "    -3.4028235e+38]\n",
      "   [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38\n",
      "    -3.4028235e+38]]]]\n",
      "Indices =\n",
      " [[[[-5 -5 -5 -5 -5]\n",
      "   [-5  5  5 -5 -5]\n",
      "   [-5  5  5 -5 -5]\n",
      "   [-5 -5 -5 -5 -5]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-02-16 18:16:48.152591937 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Y' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.152612660 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Indices' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.154548714 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:135: While parsing node number 0 [MaxPool -> \"Y\"]:\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.154584237 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "output: \"Indices\"\n",
      "name: \"\"\n",
      "op_type: \"MaxPool\"\n",
      "attribute {\n",
      "  name: \"storage_order\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 2\n",
      "  ints: 2\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"ceil_mode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  s: \"NOTSET\"\n",
      "  type: STRING\n",
      "}\n",
      "\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.154588658 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:139: --- End node ---\u001b[m\n",
      "\u001b[1;31m2026-02-16 18:16:48.154592959 [E:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2026-02-16 17:16:48   ERROR] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:151 In function emptyOutputChecker:\n",
      "[8] This version of TensorRT doesn't support mode than 1 outputs for MaxPool nodes!\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.154628826 [W:onnxruntime:Default, tensorrt_execution_provider.cc:2833 GetCapability] [TensorRT EP] No graph will run on TensorRT execution provider\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.156042996 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Y' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.156055304 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Indices' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.162276833 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Y' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n",
      "\u001b[0;93m2026-02-16 18:16:48.162290060 [W:onnxruntime:, graph.cc:120 MergeShapeInfo] Error merging shape info for output. 'Indices' source:{1,1,4,5} target:{1,1,3,4}. Falling back to lenient merge.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"\\n--- Test ONNX 'MaxPool' operator  ---\")\n",
    "print(\"\\n--- voir le valeur et indice avec CPU/CUDA double divergence autre exemple ---\")\n",
    "\n",
    "onnx_type = TensorProto.FLOAT\n",
    "\n",
    "# Input / Output shapes\n",
    "# On met une dimension H=3, W=4 pour matcher les données\n",
    "X = helper.make_tensor_value_info(\"X\", onnx_type, [1, 1, 3, 4])\n",
    "# On met des dimensions dynamiques pour laisser ONNX calculer la sortie\n",
    "Y = helper.make_tensor_value_info(\"Y\", onnx_type, [1, 1, 3, 4])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 3, 4])\n",
    "\n",
    "# MaxPool node (2x2 kernel, stride 1, padding 1)\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1],\n",
    "    ceil_mode=0\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_graph_corrige\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_corrige.onnx\")\n",
    "\n",
    "# Données d'entrée (1,1,3,4)\n",
    "data = [[[[ np.nan, np.nan, np.nan, np.nan],\n",
    "           [ np.nan, 0.0, np.nan, np.nan],\n",
    "           [ np.nan, np.nan, np.nan, np.nan]]]]\n",
    "\n",
    "x = np.array(data, dtype=np.float32)\n",
    "\n",
    "# Test pour tous les providers disponibles\n",
    "for provider in ort.get_available_providers():\n",
    "    try:\n",
    "        sess = ort.InferenceSession(\"maxpool_corrige.onnx\", providers=[provider])\n",
    "\n",
    "        outputs = sess.run(None, {\"X\": x})\n",
    "        y = outputs[0]\n",
    "        indices = outputs[1]\n",
    "\n",
    "        print(f\"\\nProvider {provider}: ✅ Test MaxPool OK\")\n",
    "        print(\"Sortie Y =\\n\", y)\n",
    "        print(\"Indices =\\n\", indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nProvider {provider}: ❌ Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c2dd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test minimal divergence NaN / Inf sur MaxPool ===\n",
      "\n",
      "--- comparer les resultats quand on appel l'operateur avec indice et sans indice, ca influence les valeur de sortie ---\n",
      "\n",
      "--- voir ce test et celui d'apres ---\n",
      "\n",
      "Entrée X =\n",
      "[[[[nan inf]\n",
      "   [-1.  1.]]]]\n",
      "\n",
      "Provider CPUExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[nan inf inf]\n",
      "   [nan inf inf]\n",
      "   [-1.  1.  1.]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[nan nan inf]\n",
      "   [nan nan inf]\n",
      "   [-1.  1.  1.]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "print(\"\\n=== Test minimal divergence NaN / Inf sur MaxPool ===\")\n",
    "print(\"\\n--- comparer les resultats quand on appel l'operateur avec indice et sans indice, ca influence les valeur de sortie ---\")\n",
    "print(\"\\n--- voir ce test et celui d'apres ---\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ Modèle MaxPool simple\n",
    "# ==========================================================\n",
    "\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [1, 1, 2, 2])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1, 1, 3, 3])\n",
    "\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1]\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_nan_inf\", [X], [Y])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_nan_inf.onnx\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ Input ciblé : NaN + +Inf\n",
    "# ==========================================================\n",
    "\n",
    "x = np.array(\n",
    "    [[[[np.nan, np.inf],\n",
    "       [-1.0, 1.0]]]],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "print(\"\\nEntrée X =\")\n",
    "print(x)\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ Exécution CPU vs CUDA\n",
    "# ==========================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for provider in [\"CPUExecutionProvider\", \"CUDAExecutionProvider\"]:\n",
    "    if provider not in ort.get_available_providers():\n",
    "        continue\n",
    "\n",
    "    sess = ort.InferenceSession(\"maxpool_nan_inf.onnx\", providers=[provider])\n",
    "    y = sess.run(None, {\"X\": x})[0]\n",
    "    results[provider] = y\n",
    "\n",
    "    print(f\"\\nProvider {provider}\")\n",
    "    print(\"Sortie Y =\")\n",
    "    print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e8b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test minimal divergence NaN / Inf sur MaxPool (avec indices) ===\n",
      "\n",
      "--- comparer les resultats quand on appel l'operateur avec indice et sans indice, ca influence les valeur de sortie ---\n",
      "\n",
      "--- voir ce test et celui d'avant des nan et des inf deviennent egale a 0---\n",
      "\n",
      "Entrée X =\n",
      "[[[[nan inf]\n",
      "   [-1.  1.]]]]\n",
      "\n",
      "Provider CPUExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[-3.4028235e+38            inf            inf]\n",
      "   [-1.0000000e+00            inf            inf]\n",
      "   [-1.0000000e+00  1.0000000e+00  1.0000000e+00]]]]\n",
      "Indices =\n",
      "[[[[-3  1  1]\n",
      "   [ 2  1  1]\n",
      "   [ 2  3  3]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [-1.  1.  1.]]]]\n",
      "Indices =\n",
      "[[[[-4 -4 -4]\n",
      "   [-4 -4 -4]\n",
      "   [ 2  3  3]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "print(\"\\n=== Test minimal divergence NaN / Inf sur MaxPool (avec indices) ===\")\n",
    "print(\"\\n--- comparer les resultats quand on appel l'operateur avec indice et sans indice, ca influence les valeur de sortie ---\")\n",
    "print(\"\\n--- voir ce test et celui d'avant des nan et des inf deviennent egale a 0---\")\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ Modèle MaxPool simple (retourne Y et Indices)\n",
    "# ==========================================================\n",
    "\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [1, 1, 2, 2])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1, 1, 3, 3])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 3, 3])\n",
    "\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1]\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_nan_inf\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_nan_inf.onnx\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ Input ciblé : NaN + +Inf\n",
    "# ==========================================================\n",
    "\n",
    "x = np.array(\n",
    "    [[[[np.nan, np.inf],\n",
    "       [-1.0, 1.0]]]],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "print(\"\\nEntrée X =\")\n",
    "print(x)\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ Exécution CPU vs CUDA\n",
    "# ==========================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for provider in [\"CPUExecutionProvider\", \"CUDAExecutionProvider\"]:\n",
    "    if provider not in ort.get_available_providers():\n",
    "        continue\n",
    "\n",
    "    sess = ort.InferenceSession(\"maxpool_nan_inf.onnx\", providers=[provider])\n",
    "    y, indices = sess.run(None, {\"X\": x})\n",
    "    results[provider] = (y, indices)\n",
    "\n",
    "    print(f\"\\nProvider {provider}\")\n",
    "    print(\"Sortie Y =\")\n",
    "    print(y)\n",
    "    print(\"Indices =\")\n",
    "    print(indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef890da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test minimal divergence NaN / Inf sur MaxPool ===\n",
      "\n",
      "Entrée X =\n",
      "[[[[nan nan]\n",
      "   [nan 1.1]]]]\n",
      "\n",
      "Provider CPUExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[           nan            nan -3.4028235e+38]\n",
      "   [           nan            nan -3.4028235e+38]\n",
      "   [           nan  1.1000000e+00  1.1000000e+00]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan 1.1]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "print(\"\\n=== Test minimal divergence NaN / Inf sur MaxPool ===\")\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ Modèle MaxPool simple\n",
    "# ==========================================================\n",
    "\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [1, 1, 2, 2])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1, 1, 3, 3])\n",
    "\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1]\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_nan_inf\", [X], [Y])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_nan_inf.onnx\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ Input ciblé : NaN + +Inf\n",
    "# ==========================================================\n",
    "\n",
    "x = np.array(\n",
    "    [[[[np.nan, np.nan],\n",
    "       [np.nan, 1.1]]]],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "print(\"\\nEntrée X =\")\n",
    "print(x)\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ Exécution CPU vs CUDA\n",
    "# ==========================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for provider in [\"CPUExecutionProvider\", \"CUDAExecutionProvider\"]:\n",
    "    if provider not in ort.get_available_providers():\n",
    "        continue\n",
    "\n",
    "    sess = ort.InferenceSession(\"maxpool_nan_inf.onnx\", providers=[provider])\n",
    "    y = sess.run(None, {\"X\": x})[0]\n",
    "    results[provider] = y\n",
    "\n",
    "    print(f\"\\nProvider {provider}\")\n",
    "    print(\"Sortie Y =\")\n",
    "    print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba4f257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test minimal divergence NaN / Inf sur MaxPool (avec indices) ===\n",
      "\n",
      "Entrée X =\n",
      "[[[[nan nan]\n",
      "   [nan 1.1]]]]\n",
      "\n",
      "Provider CPUExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]\n",
      "   [-3.4028235e+38  1.1000000e+00  1.1000000e+00]\n",
      "   [-3.4028235e+38  1.1000000e+00  1.1000000e+00]]]]\n",
      "Indices =\n",
      "[[[[-3 -3 -3]\n",
      "   [-3  3  3]\n",
      "   [-3  3  3]]]]\n",
      "\n",
      "Provider CUDAExecutionProvider\n",
      "Sortie Y =\n",
      "[[[[0.  0.  0. ]\n",
      "   [0.  0.  0. ]\n",
      "   [0.  0.  1.1]]]]\n",
      "Indices =\n",
      "[[[[-4 -4 -4]\n",
      "   [-4 -4 -4]\n",
      "   [-4 -4  3]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, save\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "print(\"\\n=== Test minimal divergence NaN / Inf sur MaxPool (avec indices) ===\")\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ Modèle MaxPool simple (retourne Y et Indices)\n",
    "# ==========================================================\n",
    "\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [1, 1, 2, 2])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1, 1, 3, 3])\n",
    "Indices = helper.make_tensor_value_info(\"Indices\", TensorProto.INT64, [1, 1, 3, 3])\n",
    "\n",
    "node = helper.make_node(\n",
    "    \"MaxPool\",\n",
    "    inputs=[\"X\"],\n",
    "    outputs=[\"Y\", \"Indices\"],\n",
    "    kernel_shape=[2, 2],\n",
    "    pads=[1, 1, 1, 1],\n",
    "    strides=[1, 1]\n",
    ")\n",
    "\n",
    "graph = helper.make_graph([node], \"maxpool_nan_inf\", [X], [Y, Indices])\n",
    "model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\n",
    "save(model, \"maxpool_nan_inf.onnx\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ Input ciblé : NaN + +Inf\n",
    "# ==========================================================\n",
    "\n",
    "x = np.array(\n",
    "    [[[[np.nan, np.nan],\n",
    "       [np.nan, 1.1]]]],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "print(\"\\nEntrée X =\")\n",
    "print(x)\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ Exécution CPU vs CUDA\n",
    "# ==========================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for provider in [\"CPUExecutionProvider\", \"CUDAExecutionProvider\"]:\n",
    "    if provider not in ort.get_available_providers():\n",
    "        continue\n",
    "\n",
    "    sess = ort.InferenceSession(\"maxpool_nan_inf.onnx\", providers=[provider])\n",
    "    y, indices = sess.run(None, {\"X\": x})\n",
    "    results[provider] = (y, indices)\n",
    "\n",
    "    print(f\"\\nProvider {provider}\")\n",
    "    print(\"Sortie Y =\")\n",
    "    print(y)\n",
    "    print(\"Indices =\")\n",
    "    print(indices)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
