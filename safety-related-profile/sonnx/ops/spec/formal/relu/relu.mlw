(** OP-ReLU Tensor Operation *)
module OPReLU
  use real.Real
  use tensor.tensor.Tensor
  use tensor.tensor.Range

  let ghost function relu (x : real) : real =
    if x < Real.(0.0) then
      0.0
    else
      x

  let ghost function drelu (x : tensor real) : data real
    = fun ks ->
      if valid ks x.dims then
        relu (x.data ks)
      else
        x.background

  let ghost function oprelu (x : tensor real) : tensor real
    ensures { result.dims = x.dims }
    ensures { result.background = x.background }
    ensures { result.data = drelu x }
    (*proof*)
    = { dims = x.dims ; data = drelu x ; background = x.background }
    (*qed*)

end

module CTensorReLU
  use tensor.std.Int
  use tensor.std.List
  use tensor.std.Clib
  use tensor.std.Cfloat
  use mach.int.Int32
  use tensor.tensor.Range
  use tensor.tensor.Tensor
  use OPReLU
  use tensor.layout.CFlat
  use tensor.libvector.CIndex
  use tensor.libtensor.CTensor

let ctensor_relu (x r : ctensor) =
  (*proof*)
  requires { valid_tensor x }
  requires { valid_tensor r }
  requires { tensor x ~= tensor r }
  (*qed*)
  ensures { tensor r = oprelu (tensor x) }

  let m = cdim_size r.t_dims r.t_rank in
  for i = 0 to m - 1 do
    (*proof*)
    invariant {
      forall k. 0 <= k < i ->
        value_at r.t_data k =
          if Real.(value_at x.t_data k < 0.0) then
            0.0
          else
            value_at x.t_data k
    }
    (*qed*)

    r.t_data[i] <-
      if Real.(x.t_data[i] .< (f32 0.0)) then
        (f32 0.0)
      else
        x.t_data[i]
  done

  (*proof*)
  ; assert { tensor r == oprelu (tensor x) }
  (*qed*)
end
