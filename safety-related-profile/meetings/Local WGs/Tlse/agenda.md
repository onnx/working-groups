
 (Availability : use the [When2Meet](https://www.when2meet.com/?34146939-oceVQ))
 
 #### 2025/02/13 

*To be completed.*

#### 2025/02/04 

- Discussion (1h max) about the specification of ops in FP. 
  - What do we need exactly?
  - Specification *vs.* implementation... Up to what point shall we be prescritive wrt the implementation?
  - The question of accuracy...
- Review of [Max](https://github.com/ericjenn/working-groups/blob/ericjenn-srpwg-wg1/safety-related-profile/sonnx/ops/spec/informal/max/max.md), [Range](https://github.com/ericjenn/working-groups/blob/ericjenn-srpwg-wg1/safety-related-profile/sonnx/ops/spec/informal/range/range.md), [Div](https://github.com/ericjenn/working-groups/blob/ericjenn-srpwg-wg1/safety-related-profile/sonnx/ops/spec/informal/div/div.md)?

#### 2025/01/15
##### Agenda
- Case of max:min/maxpool with NaN: for all operators, try to rely on the spec of Max and Min. (Nota: I don't like the spec NaN > Inf > ... I think that we should treat NaN as a special case because NaN is no normally  comparable and here we are defining a comparison...)
- Processing of existing issues: see [issues tagged "TLSE WG"](https://github.com/users/ericjenn/projects/4/views/14))
  - Clip
  - Relu
- Sujet pour CEtIC
- Interesting links identified by Jean-Loup in ONNX:
  - https://onnx.ai/onnx/repo-docs/DimensionDenotation.html
  - https://onnx.ai/onnx/repo-docs/TypeDenotation.html
  - https://onnx.ai/onnx/repo-docs/MetadataProps.html
- Status of formal specification of CONV (Mariem)
- Discussion about SONNX event in late Feb / March
##### Minutes
- review of Clip
- review of Relu
- creation of Relu
- discussion about CETiC
  
##### Actions
##### New actions
  - [X] (1501-1, Jean-Baptiste) Faire une proposition de contenu pour une soumission à  CETiC
    - Proposition faite par JB le 16/01. 
    - Modifications proposées par Eric le <19/01>
  - [ ] (1501-2, Jean-Loup) Remove the relation NaN>Inf... and replace it by an explicit test for NaNs... 
  - [X] (1501-3, Eric) Introduce the term "Scalar" and (our) concept of "type" (numerical / value type) in the glossary. 
    - Done, see [here](../../../sonnx/ops/spec/informal/common/definitions.md).
  - [X] (1501-4, eric Clarify the meaning of "heterogeneous" in ONNX
    - ONNX, heterogeneous seems to mean that the inputs can be of different types. But this is not the case. For instance, operator `Where` has "heterogeneous" arguments but, in ORT, `Where` requires the same types for `X` and `Y`. For example, one cannot mix an int32 tensor with a float tensor, or an int32 tensor with an int64 tensor. There is no type promotion. 
    - Heterogeneous **seems** to mean that the operator have arguments of different type: boolean for B and other for `X` and `Y`. 
    - But this is not the case for operator `Add` that is also tagged `heterogeneous` even though all arguments must be of the same generic type `T`
(`Add(float,int)` is rejected and so is `(int32,int64)`). 
    - Tag "Heterogeneous" is automatically generated by ONNX' documentation generator that is located in [onnx/docs/gen_doc.py](https://github.com/onnx/onnx/blob/main/onnx/defs/gen_doc.py). 
    - Normally, "heterogeneous" should only refers to variadic arguments that are not "homogeneous". The generation of "heterogeneous" appear in version ONNX 1.4.0. And it is always related to variadic arguments.  The schema is generated by function `display_schema` which calls `generate_formal_parameter_tags`:
    
    ```python
      def generate_formal_parameter_tags(formal_parameter: OpSchema.FormalParameter) -> str:
      tags: list[str] = []
    if OpSchema.FormalParameterOption.Optional == formal_parameter.option:
        tags = ["optional"]
    elif OpSchema.FormalParameterOption.Variadic == formal_parameter.option:
        if formal_parameter.is_homogeneous:
            tags = ["variadic"]
        else:
            tags = ["variadic", "heterogeneous"]
    differentiable: OpSchema.DifferentiationCategory = (
        OpSchema.DifferentiationCategory.Differentiable
    )
    non_differentiable: OpSchema.DifferentiationCategory = (
        OpSchema.DifferentiationCategory.NonDifferentiable
    )
    if differentiable == formal_parameter.differentiation_category:
        tags.append("differentiable")
    elif non_differentiable == formal_parameter.differentiation_category:
        tags.append("non-differentiable")

    return "" if len(tags) == 0 else " (" + ", ".join(tags) + ")"
    ```

  - By the way, even if the test was not only done for variadic argumebnts, it depends on the `is_homogeneous` boolean that comes from the schema defined in the `defs.cc` files (for instance, for `Where`: see [here](https://github.com/onnx/onnx/blob/main/onnx/defs/tensor/defs.cc#L2865). For the `Where` operator, the schema is the following:
    ```
    ONNX_OPERATOR_SET_SCHEMA(
    Where,
    16,
    OpSchema()
        .SetDoc(GET_OP_DOC_STR(std::string(Where_ver16_doc) + GenerateBroadcastingDocMul()))
        .Input(
            0,
            "condition",
            "When True (nonzero), yield X, otherwise yield Y",
            "B",
            OpSchema::Single,
            true, // <= This is the "homogeneous" boolean 
            1,
            OpSchema::NonDifferentiable)
        .Input(
            1,
            "X",
            "values selected at indices where condition is True",
            "T",
            OpSchema::Single,
            true,
            1,
            OpSchema::Differentiable)
        .Input(
            2,
            "Y",
            "values selected at indices where condition is False",
            "T",
            OpSchema::Single,
            true,
            1,
            OpSchema::Differentiable)
        .Output(
            0,
            "output",
            "Tensor of shape equal to the broadcasted shape of condition, X, and Y.",
            "T",
            OpSchema::Single,
            true,
            1,
            OpSchema::Differentiable)
        .TypeConstraint("B", {"tensor(bool)"}, "Constrain to boolean tensors.")
        .TypeConstraint(
            "T",
            OpSchema::all_tensor_types_ir4(),
            "Constrain input and output types to all tensor types (including bfloat).")
        .TypeAndShapeInferenceFunction([](InferenceContext& ctx) {
          propagateElemTypeFromInputToOutput(ctx, 1, 0);
          if (hasNInputShapes(ctx, 3)) {
            std::vector<const TensorShapeProto*> shapes;
            shapes.push_back(&ctx.getInputType(0)->tensor_type().shape());
            shapes.push_back(&ctx.getInputType(1)->tensor_type().shape());
            shapes.push_back(&ctx.getInputType(2)->tensor_type().shape());
            multidirectionalBroadcastShapeInference(
                shapes, *ctx.getOutputType(0)->mutable_tensor_type()->mutable_shape());
          }
        }));
    ``` 
    
      - As all boolean values are "true", "heterogeneous" should never be displayed!! 

  - [ ] (1501-5, ???) Clarify how we handle value constraints for attributes
  - [X] (1501-6,Eric) Check the actual behavior of Relu and LeakyRelu in ONNX. Check if alpha can be negative.
    - Relu: input [-1.0, 0.0, 1.0, float("nan")] =>  [ 0.  0.  1. nan]
    - Leaky relu: [-1.0, 0.0, 1.0, float("nan")] =>  [nan  0.  1. nan] with alpha = NaN, as expected. Alpha can be negative and the value for X<0 becomes positive.
  - [ ] (1501-7,???) Update the informal spec guidelines (enforce usage of ONNX names and provision of denotation, use of generic types: "with int in (int8,int16,...)" )

##### Past actions
None.